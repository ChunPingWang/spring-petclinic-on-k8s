spring:
  main:
    web-application-type: reactive
  application:
    name: genai-service
  ai:
    chat:
      client:
        enabled: true
    # These apply when using spring-ai-azure-openai-spring-boot-starter
    azure:
      openai:
        api-key: ${AZURE_OPENAI_KEY:}
        endpoint: ${AZURE_OPENAI_ENDPOINT:}
        chat:
          options:
            temperature: 0.7
            deployment-name: gpt-4o
    # These apply when using spring-ai-openai-spring-boot-starter
    openai:
      api-key: ${OPENAI_API_KEY:demo}
      chat:
        options:
          temperature: 0.7
          model: gpt-4o-mini

# GenAI Service offline mode - when true, returns fixed message without calling OpenAI
genai:
  offline-mode: ${GENAI_OFFLINE_MODE:true}

server:
  port: ${SERVER_PORT:8084}

logging:
  level:
    org:
      springframework:
        ai:
          chat:
            client:
              advisor: DEBUG

# Actuator endpoints for K8s health checks
management:
  endpoints:
    web:
      exposure:
        include: ${MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE:health,info,prometheus,metrics}
  endpoint:
    health:
      probes:
        enabled: ${MANAGEMENT_ENDPOINT_HEALTH_PROBES_ENABLED:true}
  health:
    livenessState:
      enabled: ${MANAGEMENT_HEALTH_LIVENESSSTATE_ENABLED:true}
    readinessState:
      enabled: ${MANAGEMENT_HEALTH_READINESSSTATE_ENABLED:true}
  zipkin:
    tracing:
      endpoint: ${MANAGEMENT_ZIPKIN_TRACING_ENDPOINT:http://localhost:9411/api/v2/spans}
  tracing:
    sampling:
      probability: ${MANAGEMENT_TRACING_SAMPLING_PROBABILITY:1.0}

---
spring:
  config:
    activate:
      on-profile: docker

management:
  zipkin:
    tracing:
      endpoint: http://tracing-server:9411/api/v2/spans

---
spring:
  config:
    activate:
      on-profile: kubernetes

management:
  zipkin:
    tracing:
      endpoint: ${MANAGEMENT_ZIPKIN_TRACING_ENDPOINT:http://tracing-server:9411/api/v2/spans}
